#!/bin/sh

DEFAULT_SEARCHBY=""
DEFAULT_NUMRESULTS=100
DEFAULT_SORTBY=""
DEFAULT_SORTORDER

basic_help() {
	printf "libg: Shell Script to scrape library genesis website and download books right from the terminal.\n"
	printf "USAGE: libg [OPTIONS] <search query>\n"
	printf "  OPTIONS:\n"
	printf "\t-b <value>\t'Search By': author, title, publisher, year, isbn, language, md5, tags, extension.\n"
	printf "\t-n <value>\t'Number of Results: Any positive integer. (Defaults to 100).\n"
	printf "\t-s <value>\t'Sort Results By': id, author, title, publisher, year, pages, language, filesize, extension.\n"
	printf "\t-o <value>\t'Sort Results in Order': ASC/asc (for ascending order), DESC/desc (for descending order).\n"
	printf "NOTE: -n <value> can take in any positive integer value, but can only produce results in numbers of 25, 50 or multiples of 100.\n"
	printf "      [1-25] will give 25 results, [26-50] will give 50 results, [50-100] will give 100 results,\n"
	printf "      [101-200] will give 200 results, [201-300] will give 300 results and so on.\n"
	printf "      (Obviously upto available number of search results).\n"

}

# -- PARSE OPTIONS --
while getopts b:n:s:o:h opt; do
	case "$opt" in
		# SEARCHBY: author, title, publisher, year, isbn, language, md5, tags, extension
		b) SEARCHBY="$OPTARG" ;;
		# NUMRESULTS: Number of results (Defaults to 100)
		n) NUMRESULTS="$OPTARG" ;;
		# SORTBY: id, author, title, publisher, year, pages, language, filesize, extension
		s) SORTBY="$OPTARG" ;;
		# SORTORDER: ASC, DESC
		o) SORTORDER="$OPTARG" ;;
		h) basic_help && exit 0 ;;
		\?) basic_help && exit 1 ;;
	esac
done
shift $(( OPTIND-1 ))

# Default NUMRESULTS (Number of Results) to 100
[ -z "$NUMRESULTS" ] && NUMRESULTS="$DEFAULT_NUMRESULTS"

# Convert NUMRESULTS (Number of results) to resperpage (Number of results per page) and depth (Number of search pages scraped) optimally.
# NOTE: resperpage can take values only from 25,50,100. Any other value will give 25 results.
depth=1
if [ "$NUMRESULTS" -ge 1 ] && [ "$NUMRESULTS" -le 25 ]; then
	resperpage=25
elif [ "$NUMRESULTS" -ge 26 ] && [ "$NUMRESULTS" -le 50 ]; then
	resperpage=50
elif [ "$NUMRESULTS" -ge 51 ] && [ "$NUMRESULTS" -le 100 ]; then
	resperpage=100
elif [ "$NUMRESULTS" -gt 100 ] && [ $(( "$NUMRESULTS" % 100 )) -eq 0 ]; then
	resperpage=100 && depth=$(( "$NUMRESULTS" / 100 ))
elif [ "$NUMRESULTS" -gt 100 ] && [ $(( "$NUMRESULTS" % 100 )) -gt 0 ]; then
	resperpage=100 && depth=$(( "$NUMRESULTS" / 100 + 1 ))
else
	printf "\033[31mERR\033[0m: Invalid Value for -n option! It can only take in a positive integer.\n" && exit 1
fi

# -- CHECK VALIDITY OF OPTIONS --
case "$SEARCHBY" in
	"") SEARCHBY="$DEFAULT_SEARCHBY" ;; author) ;; title) ;; publisher) ;; year) ;; isbn) ;; language) ;; md5) ;; tags) ;; extension) ;;
	*) printf "\033[31mERR\033[0m: Invalid Value for -b option!\nAllowed values: author, title, publisher, year, isbn, language, md5, tags, extension.\n" && exit 1 ;;
esac

case "$SORTBY" in
	"") SORTBY="$DEFAULT_SORTBY" ;; id) ;; author) ;; title) ;; publisher) ;; year) ;; pages) ;; language) ;; filesize) ;; extension) ;;
	*) printf "\033[31mERR\033[0m: Invalid Value for -s option!\nAllowed values: id, author, title, publisher, year, pages, language, filesize, extension.\n" && exit 1 ;;
esac

case "$SORTORDER" in
	"") SORTORDER="$DEFAULT_SORTORDER" ;; ASC) ;; DESC) ;; asc) SORTORDER="ASC" ;; desc) SORTORDER="DESC" ;;
	*) printf "\033[31mERR\033[0m: Invalid Value for -o option!\nAllowed values: ASC/asc for ascending order, DESC/desc for descending order.\n" && exit 1 ;;
esac

# -- GET SEARCH TERMS --
rm -f /tmp/libgdata.file
SEARCHTERM="$*"
if [ -z "$SEARCHTERM" ]; then
	printf "Enter searchterm: "
	read -r SEARCHTERM
fi

# -- SCRAPING --
page=1
while [ "$page" -le "$depth" ]; do
	printf "\033[1mGetting Page \033[35m$page... \033[0m"

	curl -s -G "https://libgen.is/search.php" \
		--data-urlencode "req=$SEARCHTERM" \
		--data-urlencode "column=$SEARCHBY" \
		--data-urlencode "res=$resperpage" \
		--data-urlencode "page=$page" \
		--data-urlencode "sort=$SORTBY" \
		--data-urlencode "sortmode=$SORTORDER" > /tmp/x.html
	status="$?"
	[ "$status" -eq 6 ] && printf "\033[31mFailed!\nERR\033[0m: Couldn't connect to Library Genesis! Check you internet connection and try again!\n" && exit 2

	printf "\033[32mCompleted!\033[0m\n"
	printf "\033[1mScraping Page \033[35m$page... \033[0m"
	if grep -q '<td><b>Edit</b></td></tr></tr></table>' /tmp/x.html; then
		printf "\033[1mEnd of Results! Nothing on page \033[35m$page!\033[0m\n"
		[ "$page" -eq 1 ] && exit
		break
	fi
	# Extract the relevant part. Note that we are making only one pass through the file :)
	sed -i -n '/^<td><b>Edit.*/,$p;/^<\/tr><\/table>$/q' /tmp/x.html
	# -- Cleaning up --
	# Remove unnecessary part from first line and delete last line
	sed -i '1s_.*<td_<td_;$d' /tmp/x.html
	# Delete lines of closing </tr> tags
	sed '/^\t*<\/tr>$\|^$/d' /tmp/x.html |\
	# Remove opening <td> and closing </td> tags along with \r's
	sed 's_\t*</\?td[^>]*>\r\?__g' |\
	# -- Extracting authors --
	sed 's_<a [^>]*author["'\'']>\([^<]*\)</a>_\1_g' |\
	# -- Extracting bookname --
	# Remove <font> and <i> tags first.
	# Now a author line can consist of three things: series, bookname, ISBNs separated by commas; and all of them separated by <br> tags.
	# Bookname is a must but others may or may not be there. So, convert the <br> tags to \n so that we can work on them separately.
	sed 's_</\?font[^>]*>__g' | sed 's_</\?i>__g' | sed '/<a href=['\''"]book\/index\.php/s_<br>_\n_g' |\
	# Prepend the series lines with \x01 to mark them and move them to last at the end. And remove the <a> tags around them.
	sed '/<a href=[^>]*&column=series/s_^_\x01_' | sed '/<a href=[^>]*&column=series/s_</\?a[^>]*>__g' |\
	# Prepend the book lines with \x02 to mark them. And remove the <a> tags around them.
	sed '/<a href=['\''"]book\/index\.php/s_^_\x02_' | sed '/<a href=['\''"]book\/index\.php/s_</\?a[^>]*>__g' |\
	# Prepend the ISBNs with \x03 to mark them and move them to last at the end. Also remove the </a> tags which are at the end.
	# Its the only one that starts with a space.
	sed '/^\s/s_^\s\(.*\)</a>_\x03\1_' |\
	# -- Extracting links --
	# Now remaining <a href...> tags are one with the links. Extract links and remove [edit] link which is meant for Libgen Librarian.
	sed 's_<a[^>]*href='\''\([^'\'']*\)'\''[^>]*[^<]*</a>_\1 _g' | sed '/https:\/\/library.bz\/main\/edit.*/d' |\
	# -- Cleanup --
	# Convert &amp to ',', convert <br> tags and &nbsp to spaces.
	sed 's_&amp;_,_g;s_<br>_ _g;s_&nbsp;_ _g' |\
	# Convert all newlines to tabspaces and all opening <tr> tags to newlines to separate books. Also remove trailing whitespaces.
	tr '\n' '\t' | sed 's_<tr[^>]*>_\n_g' | sed 's_\s*$__' |\
	# -- Moving Series and ISBNs to last --
	sed 's_\(\t\x01[^\t]*\)\?\(\t\x02[^\t]*\)\(\t\x03[^\t]*\)\?\(.*\)_\2\4\t\1\3_' |\
	sed 's_\t\x01_\x01_' >> /tmp/libgdata.file
	printf "\n" >> /tmp/libgdata.file
	printf "\033[32mCompleted!\033[0m\n"
	page=$(( page+1 ))
done

# -- DISPLAYING IN FZF --
# ID, Author, Title, Publisher, Year, Pages, Language, Size, Extension, Links, [Series], [ISBNs] -> 1,2,3,4,5,6,7,8,9,10,11,12
id=$(awk -F "\t" '{printf("%-30.30s | %-70.70s | %-16.16s | %-7.7s | %-4.4s | %-s\n", $2, $3, $5, $8, $9, $1)}' /tmp/libgdata.file |\
	fzf  --ansi --preview 'awk -F"\t" '\''$1 == '\''{-1}'\'' \
	{printf("\033[1m\033[31mID:\033[0m %s\
		\n\033[1m\033[31mAuthors:\033[0m %s\
		\n\033[1m\033[31mTitle:\033[0m %s\
		\n\t\033[1m\033[35mSeries:\033[0m %s\
		\n\t\033[1m\033[35mISBNs:\033[0m %s\
		\n\033[1m\033[31mPublisher:\033[0m %s\
		\n\033[1m\033[31mYear:\033[0m %s\
		\n\033[1m\033[31mPages:\033[0m %s\
		\n\033[1m\033[31mLanguage:\033[0m %s\
		\n\033[1m\033[31mSize:\033[0m %s\
		\n\033[1m\033[31mExtension:\033[0m %s", $1,$2,$3,$11,$12,$4,$5,$6,$7,$8,$9)}'\'' /tmp/libgdata.file'\
	--preview-window=up:35% |\
	awk '{print $NF}')
[ -z "$id" ] && exit 0;

# -- DOWNLOADING --
firstlink=$(awk -F"\t" '$1=="'"$id"'" {print $10}' /tmp/libgdata.file | awk '{print $1}')
curl -s "$firstlink" > /tmp/y.html
DOWNLOAD_URL=$(grep '<h2><a href=[^>]*>GET</a></h2>' /tmp/y.html | sed 's_<h2><a href=["'\'']\(.*\)["'\'']>GET</a></h2>_\1_')
wget $DOWNLOAD_URL
rm -f /tmp/libgdata.file /tmp/x.html /tmp/y.html
